{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wnezKqVQXnMA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py'\n",
      "/home/studio-lab-user\n",
      "Downloading now: taesd_decoder.pth to /tmp/vae_approx/\n",
      "File taesd_decoder.pth not changed\n",
      "Skipping file: /tmp/vae_approx/taesd_decoder.pth\n",
      "Downloading now: animagine-xl-3.0.safetensors to /tmp/checkpoints/\n",
      "File animagine-xl-3.0.safetensors not changed\n",
      "Skipping file: /tmp/checkpoints/animagine-xl-3.0.safetensors\n",
      "Downloading now: RaemuXLv1.safetensors to /tmp/checkpoints/\n",
      "File RaemuXLv1.safetensors not changed\n",
      "Skipping file: /tmp/checkpoints/RaemuXLv1.safetensors\n",
      "Downloading now: clip_vision_g.safetensors to /tmp/clip_vision/\n",
      "File clip_vision_g.safetensors not changed\n",
      "Skipping file: /tmp/clip_vision/clip_vision_g.safetensors\n",
      "Downloading now: diffusion_pytorch_model.safetensors to /tmp/vae/\n",
      "File diffusion_pytorch_model.safetensors not changed\n",
      "Skipping file: /tmp/vae/diffusion_pytorch_model.safetensors\n",
      "Downloading now: sdxl_vae.safetensors to /tmp/vae/\n",
      "File sdxl_vae.safetensors not changed\n",
      "Skipping file: /tmp/vae/sdxl_vae.safetensors\n",
      "Downloading now: gligen_sd14_textbox_pruned_fp16.safetensors to /tmp/gligen/\n",
      "File gligen_sd14_textbox_pruned_fp16.safetensors not changed\n",
      "Skipping file: /tmp/gligen/gligen_sd14_textbox_pruned_fp16.safetensors\n",
      "Downloading now: RealESRGAN_x4plus.pth to /tmp/upscale_models/\n",
      "File RealESRGAN_x4plus.pth not changed\n",
      "Skipping file: /tmp/upscale_models/RealESRGAN_x4plus.pth\n"
     ]
    }
   ],
   "source": [
    "FORCE_DOWNLOAD = False #@param {type:\"boolean\"}\n",
    "model_url = [\n",
    "\n",
    "    # Download models into folders. Uncomment necessary links.\n",
    "    # Format: (\"URL\",\"PATH\",\"*NAME (optional)\"),\n",
    "\n",
    "    # (\"URL\", \"PATH\"),\n",
    "\n",
    "    # Checkpoints\n",
    "\n",
    "### Preview Models\n",
    "    (\"https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth\", \"/tmp/vae_approx/\"),\n",
    "    # (\"https://github.com/madebyollin/taesd/raw/main/taesdxl_decoder.pth\", \"/tmp/vae_approx/\"),\n",
    "    # (\"https://github.com/madebyollin/taesd/raw/main/taesd_decoder.pth\", \"/tmp/vae_approx/\"),\n",
    "    # (\"https://github.com/madebyollin/taesd/raw/main/taesdxl_decoder.pth\", \"/tmp/vae_approx/\"),\n",
    "\n",
    "### SDXL\n",
    "### I recommend these workflow examples: https://comfyanonymous.github.io/ComfyUI_examples/sdxl/\n",
    "\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    (\"https://huggingface.co/cagliostrolab/animagine-xl-3.0/resolve/main/animagine-xl-3.0.safetensors?download=true\", \"/tmp/checkpoints/\"),\n",
    "    (\"https://huggingface.co/Raelina/RaemuXL/resolve/main/RaemuXLv1.safetensors?download=true\", \"/tmp/checkpoints/\"),\n",
    "# SDXL ReVision\n",
    "    (\"https://huggingface.co/comfyanonymous/clip_vision_g/resolve/main/clip_vision_g.safetensors\", \"/tmp/clip_vision/\"),\n",
    "\n",
    "# SD1.5\n",
    "    # (\"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\", \"/tmp/checkpoints/\"),\n",
    "\n",
    "# SD2\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors\", \"/tmp/checkpoints/\"),\n",
    "\n",
    "# Some SD1.5 anime style\n",
    "    # (\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors\", \"/tmp/checkpoints/\"),\n",
    "\n",
    "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
    "    # (\"https://huggingface.co/waifu-diffusion/wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors\", \"/tmp/checkpoints/\"),\n",
    "\n",
    "\n",
    "# unCLIP models\n",
    "    # (\"https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors\", \"/tmp/checkpoints/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors\", \"/tmp/checkpoints/\"),\n",
    "\n",
    "\n",
    "# VAE\n",
    "    # (\"https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/vae/\", \"sd-vae-ft-mse.safetensors\"),\n",
    "    # (\"https://huggingface.co/stabilityai/sd-vae-ft-ema/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/vae/\", \"sd-vae-ft-ema.safetensors\"),\n",
    "    # (\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors\", \"/tmp/vae/\"),\n",
    "    # (\"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt\", \"/tmp/vae/\"),\n",
    "    (\"https://huggingface.co/cagliostrolab/animagine-xl-3.0/resolve/main/vae/diffusion_pytorch_model.safetensors?download=true\", \"/tmp/vae/\"),\n",
    "# SDXL VAE\n",
    "    (\"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\", \"/tmp/vae/\"),\n",
    "\n",
    "# IP-Adapter (downloading to 'ComfyUI_IPAdapter_plus' node folder)\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_light.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "# IP-Adapter SDXL\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.bin\", \"/tmp/custom_nodes/ComfyUI_IPAdapter_plus/models\"),\n",
    "# Image Encoder (for IP-Adapter)\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors\", \"/tmp/clip_vision/\", \"image_encoder_model.safetensors\"),\n",
    "# Image Encoder SDXL (for IP-Adapter)\n",
    "    # (\"https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors\", \"/tmp/clip_vision/\", \"image_encoder_sdxl_model.safetensors\"),\n",
    "       \n",
    "    \n",
    "# Loras\n",
    "    # (\"https://civitai.com/api/download/models/10350\", \"/tmp/loras/\", \"theovercomer8sContrastFix_sd21768.safetensors\"), #theovercomer8sContrastFix SD2.x 768-v\n",
    "    # (\"https://civitai.com/api/download/models/10638\", \"/tmp/loras/\", \"theovercomer8sContrastFix_sd15.safetensors\"), #theovercomer8sContrastFix SD1.x\n",
    "    # (\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/\", \"/tmp/loras/\", \"sd_xl_offset_example-lora_1.0.safetensors\"), #SDXL offset noise lora\n",
    "\n",
    "\n",
    "# T2I-Adapter\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth\", \"/tmp/controlnet/\"),\n",
    "\n",
    "# T2I-Adapter SDXL\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-canny-sdxl-1.0.safetensors\"),\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-sketch-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-sketch-sdxl-1.0.safetensors\"),\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-openpose-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-openpose-sdxl-1.0.safetensors\"),\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-lineart-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-lineart-sdxl-1.0.safetensors\"),\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-depth-midas-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-depth-midas-sdxl-1.0.safetensors\"),\n",
    "    # (\"https://huggingface.co/TencentARC/t2i-adapter-depth-zoe-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors\", \"/tmp/controlnet/\", \"t2i-adapter-depth-zoe-sdxl-1.0.safetensors\"),\n",
    "\n",
    "# T2I Styles Model\n",
    "    # (\"https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth\", \"/tmp/style_models/\"),\n",
    "\n",
    "# CLIPVision model (needed for styles model)\n",
    "    # (\"https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin\", \"/tmp/clip_vision/\", \"clip_vit14.bin\"),\n",
    "\n",
    "\n",
    "# Control-lora stabilityai\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-canny-rank128.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-depth-rank128.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-recolor-rank128.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank128/control-lora-sketch-rank128-metadata.safetensors\", \"/tmp/controlnet/\"),\n",
    "\n",
    "# ControlNet\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors\", \"/tmp/controlnet/\"),\n",
    "\n",
    "# ControlNet SDXL\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors\", \"/tmp/controlnet/\"),\n",
    "    # (\"https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors\", \"/tmp/controlnet/\"),\n",
    "\n",
    "# GLIGEN\n",
    "    (\"https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors\", \"/tmp/gligen/\"),\n",
    "\n",
    "# ESRGAN upscale model\n",
    "    (\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\", \"/tmp/upscale_models/\"),\n",
    "    # (\"https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth\", \"/tmp/upscale_models/\"),\n",
    "    # (\"https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth\", \"/tmp/upscale_models/\"),\n",
    "]\n",
    "\n",
    "# Controlnet Preprocessor nodes by Fannovel16\n",
    "#%cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "for _, model in enumerate(model_url):\n",
    "    model_url = model[0]\n",
    "    model_dir = model[1]\n",
    "    model_name = \"\"\n",
    "    \n",
    "    try:\n",
    "       response = requests.get(model_url, stream=True)\n",
    "    except:\n",
    "       print(\"Unknown error.\")\n",
    "    \n",
    "      \n",
    "    if len(model) > 2:\n",
    "        model_name = model[2]\n",
    "    elif 'content-disposition' in response.headers:\n",
    "        d = response.headers['content-disposition']\n",
    "        model_name = re.findall(\"filename=(.+)\", d)[0].strip(\"\\\"';\")\n",
    "    \n",
    "        \n",
    "    if not model_name:      \n",
    "        model_name = model_url.split(\"/\")[-1]\n",
    "    \n",
    "    file_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "\n",
    "    print(f\"Downloading now: {model_name} to {model_dir}\")\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    file_changed = True\n",
    "    \n",
    "    if file_exists and not FORCE_DOWNLOAD:\n",
    "        if os.path.getsize(file_path) == total_size_in_bytes:\n",
    "            file_changed = False\n",
    "            print(f\"File {model_name} not changed\")\n",
    "\n",
    "    if not file_exists or file_changed or FORCE_DOWNLOAD:\n",
    "        block_size = 1024 #1 Kibibyte\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "        if total_size_in_bytes and progress_bar.n != total_size_in_bytes:\n",
    "            print(f\"ERROR downloading {model_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping file: {file_path}\")\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
